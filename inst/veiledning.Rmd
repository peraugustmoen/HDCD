---
title: "veiledning"
author: "Per August Jarval Moen"
date: "4/5/2022"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Setup
We observe $p$-dimensional Gaussian vectors
$$
X_i \sim \text{N}(\mu_i, I_p),$$
for $i = 1, ..., n$. 
\newline\newline
There are $K$ change-points $\eta_1, \ldots, \eta_K$.  At the $v$-th change-point $\eta_v$, denote the change by 
$$
\theta_v = \mu_{\eta_v + 1} - \mu_{\eta_v}.
$$


## Minimax testing
Suppose $K=1$. Let $(s,e] = [1, \ldots, \eta_1, \ldots, n]$.  Let $\Delta_1 = \min(\eta_1, n-\eta_1)$.
\newline\newline
Let $s = \|\theta_1\|_0$, i.e. the number of components that undergo a change at the change-point. 
\newline\newline
If $s \leq \sqrt{p \log(n)}$ we call the change sparse, and dense otherwise. 
\newline\newline
The minimax testing "rate" for a change-point in $(s,e]$ at any location is
$$
\Delta_1 \|\theta_1\|^2 \gtrsim \begin{cases} s \log\left(\frac{e p \log(n)}{s^2}\right) \vee \log(n), \text{ if sparse}\\ \sqrt{p \log n} \quad \quad \quad \quad \quad \quad  \ \ ,\text{if dense} \end{cases}
$$
At $s = \sqrt{p \log n}$ the rates are the same. 

## Setup
For any segment $(s,e] = [s+1, e]$ and any $b \in [s+1, ..., e-1]$, let
$$\mathcal{C}^b_{(s,e]}(X_j) = \text{CUSUM}_{(s,e]}^b(X_j).$$
(Cusum of $j$-th component, ie weighted difference in mean between $X_{j, s+1}, \ldots, X_{j,b}$ and $X_{j,b+1}, \ldots, X_{j, e}$).
\newline\newline
If there is no change-point in $(s,e]$, then 
$$
\mathcal{C}^b_{(s,e]}(X_j) \overset{i.i.d.}{\sim} \text{N}(0,1),
$$
for $j=1, \ldots, p$. 

## Test statistic
For any sparsity level $t$, let
$$
a(t)^2 :=  \begin{cases}  4\log\left(\frac{e p \log(n)}{t^2}\right), \text{if } t < \sqrt{p \log (n)}\\ 0 \quad \quad \quad \quad \quad \ \ \  ,\text{else} \end{cases}
$$
$$
T^b_{(s,e]}(t) := \sum_{j=1}^p \left( \mathcal{C}^b_{(s,e]}(X_j)^2 - \nu_{a(t)} \right)I_{\left\{\left| \mathcal{C}^b_{(s,e]}(X_j) \right| > a(t) \right\}},
$$
where $\nu_a = \mathbb{E}\left(Z^2 \ | \ |Z|>a \right)$, $Z \sim \text{N}(0,1)$. 

## Behavior
If there is no change, then with probability greater than
$$1- \frac{1}{n},$$
for all $b$, and all sparsity levels t, we have
$$
T^b_{(s,e]}(t)< \widetilde{C} \begin{cases} t \log\left(\frac{e p \log(n)}{t^2}\right) \vee \log(n), \text{ if } t < \sqrt{p \log n}\\ \sqrt{p \log n} \quad \quad  \quad \quad \quad \quad \ \ ,\text{else} \end{cases}
$$

## Behavior
If there is a change at $\eta_1$, with sparsity level $s$, and the minimax rate is "satisfied", then 
$$
T^{\eta_1}_{(s,e]}(s)> \widetilde{C} \begin{cases} s \log\left(\frac{e p \log(n)}{s^2}\right) \vee \log(n) , \text{ if } s < \sqrt{p \log n}\\ \sqrt{p \log n} \quad \quad \quad \quad \quad \quad \ \ ,\text{else}, \end{cases}
$$
also with probability larger than $1-\frac{1}{n}$. 

## Remark
For the dense case, as long as $p\geq \log(n)$, the minimum requirement for testing existence of a change-point is the same as the condition for consistenlty estimating the location. 

## Idea for change-point localization
- Generate seeded intervals $(s_k, e_k]$
- Iterating through the intervals sorted by length (shortest to longest), stop once you find an interval $(s_k, e_k]$, a position $b$ and sparsity $t$ such that
$$
T^{b}_{(s_k,e_k]}(t)> C_1 \begin{cases} t \log\left(\frac{e p \log(n)}{t^2}\right) \vee \log(n) , \text{ if } t < \sqrt{p \log n}\\ \sqrt{p \log n} \quad \quad \quad \quad \quad \quad \ \ ,\text{else}, \end{cases}.
$$
\newline\newline
Then estimate the change-point to have localization 
$$b^* = \arg \max_b \ \max_t T^b_{(s_k,e_k]} -  C_1 \begin{cases} t \log\left(\frac{e p \log(n)}{t^2}\right) \vee \log(n) , \text{ if } t < \sqrt{p \log n}\\ \sqrt{p \log n} \quad \quad \quad \quad \quad \quad \ \ ,\text{else}, \end{cases}.$$

## Goal
Goal is this: If all change-points $\eta_j$ satisfy
$$
\Delta_j \|\theta_j\|^2 \geq C_0 \begin{cases} s_j \log\left(\frac{e p \log(n)}{s_j^2}\right) \vee \log(n), \text{ if sparse}\\ \sqrt{p \log n} \quad \quad \quad \quad \quad \quad  \ \ ,\text{if dense} \end{cases},$$
then with probablity larger than $1-\frac{1}{n}$, 

- $\widehat{K} = K$, 
- for all $j$, $| \widehat{\eta_j} - \eta_j| \leq C_3 \xi(s)$, 
where 
$$\xi(s) = \begin{cases}\log\left(\frac{e p \log(n)}{t^2}\right) \vee \log(n) , \text{ if } t < \sqrt{p \log n}\\ \sqrt{p \log n} \quad \quad \quad \quad \quad \quad \ \ ,\text{else}, \end{cases}$$ 

## Comparison
For $p=1$ this is the same as PELT, Seeded BS, and minimax rate optimal. For $p \geq \log(n)$, and only dense change-points, the above is already proven. 

## The problem
... is essentially controlling 
\begin{align}
T^b_{(s,e]}(t) :&= \sum_{j=1}^p \left( \mathcal{C}^b_{(s,e]}(X_j)^2 - \nu_{a(t)} \right)I_{\left\{\left| \mathcal{C}^b_{(s,e]}(X_j) \right| > a(t) \right\}},
\\
&\overset{d}{=}
\sum_{j=1}^p \left( \left(\mathcal{C}^b_{(s,e]}(\mu_j) + Z_j\right)^2 - \nu_{a(t)} \right)I_{\left\{\left| \mathcal{C}^b_{(s,e]}(\mu_j) + Z_j \right| > a(t) \right\}}
\end{align}
when $(s,e]$ contains a change-point. Need: 

- If $(s,e]$ contains an un-detected change-point, and if for some sparsity level $t$, $T^b_{(s,e]}(t)>C_1 \xi(t)$, then $b$ must be close to a change-point. 
- When $(s,e]$ contains a detected change-point (close to $s$ or $e$), then for all $t$, $T^b_{(s,e]}(t)<C_1 \xi(t)$. 

## Simulations: 
Better than Inspect based on Seeded BS, especially for dense change-points.

